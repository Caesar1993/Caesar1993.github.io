# 基础信息

 姓	**名： 张学渊**					**性**	**别： 男**										**籍**	**贯：**	内蒙古  		**出生年月： 1993.12**

 手机/微信:13520572037		**邮箱：**[904690437@163.com ](mailto:904690437@163.com)		**学**	**历： 本科**				**居住地：昌平回龙观**

# 教育经历

## *吉林大学       **					**2011.8-2015.06*

> **全日制本科**					**通信工程**			**学士    **

## *北京航空航天大学 **	**		**2022.8-2025.06***

> **非全日制研究生**			**软件工程**			**硕士     **

# 专业技能

> 1. **精通Python、linux远程开发，熟练掌握C/C++/C#语言**
> 2. **精通PyTorch和Transformer结构，掌握Megatron/ColossalAI大模型训练框架，Energonai大模型推理框架**
> 3. **有大模型训练&推理经验，擅长利用prompt调动大模型能力解决工作问题**
> 4. **完整的NLP体系知识，包括word2vec、分类匹配、实体抽取、知识图谱等，掌握多种机器学习算法**
> 5. **个人 Git地址：**[https://gitee.com/michaelzhang93/projects](https://gitee.com/michaelzhang93/projects)，[https://github.com/Caesar1993?tab=repositories](https://github.com/Caesar1993?tab=repositories)

# 项目经历：

## *项目 1.大模型并行推理*

* **实施时间：2023/5-至今**
* **项目描述：**

  1. **LLM推理占用显存多、推理时间长，不利于低资源部署**
  2. **最终基于Energonai框架自主实现了chatglm-6b的并行推理**
* **关键步骤：**

  1. **调研模型并行策略，包括tensor并行、pipeline 并行**
  2. **调研模型并行框架，包括Energonai、Fast transformer等**
  3. **使用Energonai框架，多卡并行推理opt-66b模型，推理速度是未做模型并行的3倍以上**
  4. **想要实现chatglm结构的并行推理，需掌握mix_qkv拆分、ROPE处理、init层定义、forward推理**

## *项目 2.医学大模型训练*

* **实施时间：2023/2-至今**
* **项目描述：**

  1. **在医学的垂直领域上训练百亿级参数的大模型，实现智能问诊**
  2. **训练数据是1亿以上的医患互联网对话**
* **关键步骤：**

  1. **完成单轮对话和多轮对话的数据清洗，筛选后训练数据为5000万+的医患对话**
  2. **为提升多机多卡的训练效率，采用Megatron框架、数据分桶、梯度累计更新等加速策略**
  3. **baseline选择GPT2作为基础模型，调研GLM、LLaMA等大模型的训练方式**

## *项目3. 大模型微调-长文本分类*

* **实施时间：2023/4-至今**
* **项目描述：**
  1. **由于Bert模型能处理的token有限，相关技术中对于长文本分类任务准确率和召回率均较低，长文本指长度为2000字以上的文章**
  2. **各个类别的准确率和召回率均需要达到80%左右，方可符合业务需求**
* **关键步骤：**
  1. **调研主流大模型的能力边界，通过阅读p-Tuning论文，了解到glm模型具备一定的NLU能力，决定采用chatglm-6b模型作为base模型**
  2. **在THUNS公开数据集上，采集其中文本长度在2000-4000字的数据进行实验，训练数据为3万条**
  3. **采用p-Tuning的方式对chatglm-6b进行微调，在5000条的测试集上有9个类别的F1值高于80%，剩余3个类别的F1值也在75%以上**

## *项目 4.论文PDF的图表抽取*

* **实施时间：2022/12-2022/2**
* **项目描述：**
  1. **论文PDF中有大量含有重要信息的图和表，将其抽取出来展示给科研人员，提升科研人员获取信息的效率**
  2. **项目在我接手前，图和表格抽取的准确率和召回率为60%左右，为不可用状态**
* **关键步骤：**
  1. **在我研究了论文PDF的格式后，弃用神经网络的方式，改用流数据处理和规则处理的方式**
  2. **采用PyMuPDF识别流数据，规则是逐条发现的，例如表名在表上方，以表名的检测来触发横线的检测，横线可能是表格的组成部分等等**
  3. **截止目前，以每个图或表的角度进行统计，表格抽取的准召在95%以上，达到可以商用的状态，图像抽取的准确率在85%以上，均遥遥领先竞品**

## *项目 5.医学机构简称和全称提取*

* **实施时间：2022/10-2022/12**
* **项目描述：**
  1. **项目有百万条医学论文数据，其中包含wos/scopus/deminson三个来源的论文发表机构的信息，wos来源记录的是机构简称，其他两个来源是机构全称但有大量缺失，项目要求输出每条论文的机构全称**
* **关键步骤：**
  1. **原始数据的机构信息包含有机构名称和地址，需要通过二分类模型将机构名称和地址分割开来**
  2. **基于不同来源的机构名称的空格信息，将机构信息部分内容的英文简称和英文全称进行对应**
  3. **对细分规则进行优化，最终产出高质量的14万对英文简称和全称来支持项目交付**

## *项目 6.美国专利文本匹配*

* **实施时间：2022/10-2022/12**
* **项目描述：**
  1. **该项目是最新的kaggle比赛，有34k标注数据，label是两个专利短语的相似得分**
  2. **比赛要求以模型结果和label的pearson相关系数作为评价指标**
* **关键步骤：**
  1. **发掘训练数据中的语义信息，将两个待匹配短语与领域信息结合起来，例如化学领域，丰富模型的输入;**
  2. **搭建了文本匹配和文本分类两套框架来实现该任务，使用了deberta/bert预训练模型;**
  3. **文本匹配使用CosineSimilarity拟合label的分数，采用MSEloss作为损失函数;**
  4. **文本分类最终用Linear函数将结果映射到5维向量，采用CrossEntropyloss作为损失函数;**
  5. **最终在训练集上的pearson系数能达到0.79，kaggel上的金牌方案是0.87。**

## *项目 7.多标签分类*

* **实施时间：2022/6-2022/9**
* **项目描述：**
  1. **识别销售聊天中的关键事件，如客户满意点，抱怨点等**
  2. **有监督训练，初始标签是基于规则和人工标注获取的**
* **关键步骤：**
  1. **数据采样过程中尽可能保证分布均衡 ，并使用cleanlab评测label质量**
  2. **将长会话经过QA模型或者多轮对话的方式分成短对话，然后再识别**
  3. **调用 FastText和bert模型，在6个类别的f1值达到了80%以上，已上线部署。**

## *项目 8.基于知识图谱的问答*

* **实施时间：2021/12-2022/1**
* **项目描述：**
  1. **将周杰伦的作品信息以三元组形式存储，然后写成 cypher 语句，存入 neo4j**
  2. **然后用正则提取自然语言中的实体，属性，标签等值**
  3. **将提取到的信息插入模板，模板中定义有 cypher 语句，然后查询得到结果**
* **关键步骤：**
  1. **抓取实体列表，构建三元组，基于某种规则从三元组文本中分离出实体，属性，标签**
  2. **在与模板匹配的问题，可以使用 Jaccard 距离，编辑距离，神经网络等多种方法**
  3. **思考升级为 NER 模型提取实体，也可以直接用模型生成 cypher 语句**

# 工作经历

## 一. 2022/10 –至今**	**         中图科信数智

**职位：NLP算法**

**工作成果：**

1. **实现660亿参数的大模型并行推理，实现chatglm-6B的多卡并行推理**
2. **搭建10B级别大模型训练框架**
3. **从论文PDF中抽取表格和图像，表格抽取的准召达到95%以上，图像抽取的准召达到80%以上**
4. **分析医学论文的地址数据的规则，利用神经网络和规则抽取出常见机构简称全称对应关系14万对**

## *二. 2022/06 –2022/09**	**         深维智信*

**职位：算法实习**

**工作成果：**

1. **识别销售会话中的关键事件，模型包括fasttext、bert等**
2. **编写用于评测ASR效率的统计工具**
3. **完成千万级的对话数据的清洗、处理工作**

（在此之前，从AI项目经理工作）

# 自 我 评 价

**善于沟通**		**逻辑力强**		**责任心强**		**学习力强**
